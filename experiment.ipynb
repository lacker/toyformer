{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9c9e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time\n",
    "import makemore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f505a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_digit():\n",
    "    return random.choice(\"0123456789\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b48c1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2580110852'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def palindrome():\n",
    "    start_letters = [random_digit() for _ in range(5)]\n",
    "    end_letters = list(reversed(start_letters))\n",
    "    return \"\".join(start_letters + end_letters)\n",
    "make_palindrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79533c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 10000\n",
      "max word length: 10\n",
      "number of unique characters in the vocabulary: 10\n",
      "vocabulary:\n",
      "0123456789\n",
      "split up the dataset into 9000 training examples and 1000 test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = makemore.generate_datasets(palindrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ee12de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset determined that: vocab_size=11, block_size=11\n",
      "number of parameters: 0.20M\n",
      "model #params: 202176\n"
     ]
    }
   ],
   "source": [
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_output_length()\n",
    "print(f\"dataset determined that: {vocab_size=}, {block_size=}\")\n",
    "\n",
    "config = makemore.ModelConfig(vocab_size=vocab_size, block_size=block_size)\n",
    "model = makemore.Transformer(config)\n",
    "model.to(\"cuda\")\n",
    "print(f\"model #params: {sum(p.numel() for p in model.parameters())}\")\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.01\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay,\n",
    "                              betas=(0.9, 0.99), eps=1e-8)\n",
    "batch_loader = makemore.InfiniteDataLoader(train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "                                           num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f339e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 1.0588 | step time 14.16ms\n",
      "step 10 | loss 1.0470 | step time 3.29ms\n",
      "step 20 | loss 1.0520 | step time 3.29ms\n",
      "step 30 | loss 1.0631 | step time 3.97ms\n",
      "step 40 | loss 1.0498 | step time 3.23ms\n",
      "step 50 | loss 1.0523 | step time 3.43ms\n",
      "step 60 | loss 1.0563 | step time 3.76ms\n",
      "step 70 | loss 1.0473 | step time 3.22ms\n",
      "step 80 | loss 1.0535 | step time 3.28ms\n",
      "step 90 | loss 1.0493 | step time 3.78ms\n",
      "step 100 | loss 1.0465 | step time 3.17ms\n",
      "step 110 | loss 1.0542 | step time 3.27ms\n",
      "step 120 | loss 1.0511 | step time 3.25ms\n",
      "step 130 | loss 1.0517 | step time 3.75ms\n",
      "step 140 | loss 1.0586 | step time 3.21ms\n",
      "step 150 | loss 1.0511 | step time 3.20ms\n",
      "step 160 | loss 1.0522 | step time 3.19ms\n",
      "step 170 | loss 1.0575 | step time 3.33ms\n",
      "step 180 | loss 1.0652 | step time 3.18ms\n",
      "step 190 | loss 1.0551 | step time 3.17ms\n",
      "step 199 train loss: 1.0521115064620972 test loss: 1.0521448850631714\n",
      "--------------------------------------------------------------------------------\n",
      "0 samples that are in train:\n",
      "0 samples that are in test:\n",
      "10 samples that are new:\n",
      "3462662643\n",
      "6260110626\n",
      "6020220206\n",
      "8286776828\n",
      "4364554634\n",
      "3326776233\n",
      "2925885292\n",
      "8627997268\n",
      "7797227977\n",
      "7852662587\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for step in range(200):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # get the next batch, ship to device, and unpack it to input and target\n",
    "    batch = batch_loader.next()\n",
    "    batch = [t.to(\"cuda\") for t in batch]\n",
    "    X, Y = batch\n",
    "\n",
    "    # feed into the model\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # calculate the gradient, update the weights\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for all CUDA work on the GPU to finish then calculate iteration time taken\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    # logging\n",
    "    if step % 10 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.4f} | step time {(t1-t0)*1000:.2f}ms\")\n",
    "\n",
    "# evaluate the model\n",
    "train_loss = makemore.evaluate(model, train_dataset, batch_size=100, max_batches=10)\n",
    "test_loss = makemore.evaluate(model, test_dataset, batch_size=100, max_batches=10)           \n",
    "print(f\"step {step} train loss: {train_loss} test loss: {test_loss}\")\n",
    "                \n",
    "# sample from the model\n",
    "makemore.print_samples(model, train_dataset, test_dataset, num=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0831f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
